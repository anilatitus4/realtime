steps:
  # Step 1: Terraform - Infra Setup
  - name: 'hashicorp/terraform:1.6.6'
    id: Terraform-Apply
    dir: 'terraform'  # <--- Folder where your main.tf is
    entrypoint: sh
    args:
      - -c
      - |
        terraform init
        terraform apply -auto-approve

  # Step 2: Build Dataflow Template (Python script based)
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: Dataflow-Stage-Job
    entrypoint: bash
    args:
      - -c
      - |
        gcloud dataflow flex-template build gs://$_DATAFLOW_TEMPLATE_BUCKET/$_DATAFLOW_TEMPLATE_FILE \
          --image=gcr.io/$PROJECT_ID/dataflow-pipeline:latest \
          --sdk-language=PYTHON \
          --flex-template-base-image=PYTHON3 \
          --metadata-file=metadata.json \
          --project=$PROJECT_ID

  # Step 3: Run the Dataflow Job from Template
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: Run-Dataflow-Job
    entrypoint: bash
    args:
      - -c
      - |
        gcloud dataflow flex-template run "realtime-streaming-job-$(date +%s)" \
          --template-file-gcs-location=gs://$_DATAFLOW_TEMPLATE_BUCKET/$_DATAFLOW_TEMPLATE_FILE \
          --region=us-central1 \
          --parameters inputSubscription=projects/$PROJECT_ID/subscriptions/realtime-stream-sub \
          --project=$PROJECT_ID

  # Step 4: Deploy Cloud Run (already built image)
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: Deploy-Cloud-Run
    entrypoint: gcloud
    args:
      [
        'run',
        'deploy',
        'publisher-service',
        '--image',
        'gcr.io/$PROJECT_ID/publisher-service:latest',
        '--region',
        'us-central1',
        '--platform',
        'managed',
        '--allow-unauthenticated'
      ]

timeout: 1200s

substitutions:
  _DATAFLOW_TEMPLATE_BUCKET: "realtime-data-pipeline-123-staging"  # <-- Replace if different
  _DATAFLOW_TEMPLATE_FILE: "pipeline-template.json"                 # <-- Update if different

options:
  logging: CLOUD_LOGGING_ONLY
