steps:
  # 1. Terraform init + apply
  - name: hashicorp/terraform:1.6.6
    dir: terraform
    entrypoint: sh
    args:
      - -c
      - |
        terraform init
        terraform apply -auto-approve

  # 2. Build & push Cloud Run container (publisher)
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-f', './publisher/Dockerfile', '-t', 'gcr.io/$PROJECT_ID/publisher-service:latest', './publisher']

  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/$PROJECT_ID/publisher-service:latest']

  # 3. Deploy Cloud Run
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: gcloud
    args:
      [
        'run', 'deploy', 'publisher-service',
        '--image', 'gcr.io/$PROJECT_ID/publisher-service:latest',
        '--platform', 'managed',
        '--region', 'us-central1',
        '--allow-unauthenticated'
      ]

  # Step 4: Build and push Docker image for Dataflow
  - name: 'gcr.io/cloud-builders/docker'
    dir: dataflow-pipeline
    args: ['build', '-t', 'gcr.io/$PROJECT_ID/dataflow-pipeline:latest', '.']

  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/$PROJECT_ID/dataflow-pipeline:latest']

  # Step 5: Build Dataflow Flex Template
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: bash
    args:
      - -c
      - |
        gcloud dataflow flex-template build gs://realtime-data-pipeline-123-dataflow-template/bigquery-template.json \
          --image=gcr.io/$PROJECT_ID/dataflow-pipeline:latest \
          --sdk-language=PYTHON \
          --metadata-file=metadata.json \
          --temp-location=gs://realtime-data-pipeline-123-staging/temp

timeout: 1800s

options:
  logging: CLOUD_LOGGING_ONLY
